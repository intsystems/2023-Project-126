\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}

\renewcommand{\headeright}{Draft}
\renewcommand{\undertitle}{Draft}

\title{Detection of machine-generated fragments in text}

\author{ Anastasiya Voznyuk \\
	Department of Applied Informatics and Mathematics\\
	Moscow Institute of Physics and Technology\\
	\texttt{vozniuk.ae@phystech.edu} \\
	%\texttt{\href{https://colab.research.google.com/drive/1HBB8002s5_tFgFxGzIZQcBmsOsW6YcMh?usp=sharing} {Project code}} \\
    \\
    \textbf{Andrey Grabovoy} \\
	Moscow Institute of Physics and Technology\\
    Antiplagiat Company \\
	%% Address \\
	\texttt{grabovoy@ap-team.ru}}
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\

\date{}

\renewcommand{\shorttitle}{Detection of machine-generated fragments in text}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Voznyuk Anastasiya},
pdfkeywords={Detection},
}

\begin{document}
\maketitle

\begin{abstract}
	This paper considers the problem of detecting machine-generated parts of text in the document. We introduce a model, that can detect such text fragments and distinguish their origin among several language models. To solve this problem we combine two models. The objective of the first one is to divide document into fragments of human-written text and machine-generated text. The second model aims to classify the generated fragments according to the language model from which they were obtained. In the computational experiment, we analyse the quality of such approach on dataset of documents with fragments generated by GPT-2 and GPT-3.
\end{abstract}


\keywords{Text Generation, Detection of Machine-generated text, Neural Authorship Attribution, Machine Learning, Natural Language Processing}

\section{Introduction}

% \cite{EVL:1}
In recent years there has been a rapid development of language models for text generation, transformers in particular, for example GPT\cite{gpt}, GPT-2\cite{gpt2}, GPT-3\cite{gpt3}, CTRL\cite{ctrl} and BERT\cite{bert}. Problem of detecting machine-generated text has become more relevant recently due to the released ChatGPT model by OpenAI. This model was learnt on massive amounts of data and now is able to provide texts that are hardly distinguishable from human texts. The opposite task of detecting machine-generated text becomes more important due to many possible malicious usages of this technology. One can analyse the whole text or fragments of text.
We will focus on methods of subtracting the text-generated fragments in the set of documents.\\
With increased computational resources past statistical methods were applied to the novel detection problem. One of them was prediction entropy as an indicator of fake text, as described in \cite{relativeentropy}. Also perplexity\cite{perplexity} of the text or frequency of rare bigrams \cite{rare_bigrams} or value of tf-idf \cite{solaiman} can be taken into account. Another approach is to use classifiers that try to label given texts \cite{Kuznetsov}.\\
For a long period recursive neural networks like LSTM were showing best results at solving the problem of detection. At 2018 the mechanism of self-attention\cite{Vaswani} was introduced and transformers have become new state-of-the-art approach.  Every new model has its own features and is usually learnt on larger amounts of data, but attention mechanism always stays. These model can be used in detection of machine-generated text in two ways\cite{solaiman}. The first method is using a language model that searches for artefacts from  methods, which most models are using for generating concise texts \cite{gltr}. Additional training on new data is not required. The second method is fine-tuning based detection: one fine-tunes a language model to “detect itself” with using some stochastic methods, for example top-k, top-p sampling etc.\\
The first step is to divide each document into non-overlapping blocks of sentences. We take the baseline from work\cite{Kuznetsov} on plagiarism detection. Also metrics and construction of loss function from the works about text segmentation problem can be borrowed. We suppose, that within a fragment with artificial text there is a finished idea and similar semantic structure. For example, loss function is described at \cite{ts_loss}, and metrics are described at \cite{ts_metrics}. The next step is to detect the origin of machine-generated text and this can be done with usage of transformer-based models. This papers presents computational experiments with these models to combine them in one model and analyze the parsmets on which the best performance will be obrained. Dataset will be constructed from existing datasets, such as GPT-2 output dataset\cite{gpt2-dataset} and RUaTD competition dataset\cite{ruatd-dataset}.  %%%%---!!
\section{Problem statement}

Let $\mathbf{W}$ be our alphabet, tokens consists from characters from that alphabet.

Let $$\mathbb{D} = \{[t_j] : t_j \in \mathbf{W}\}$$ be the space of our documents.

We have set of $n$ documents
$$\mathbf{D} = \bigcup_{i=1}^{n}D^i, D^i \in \mathbb{D}$$

%Each document  $D^i$ consists of $d_i$ tokens $t_1^i, t_2^i, ..., t_{d_i}^i$, where $t_j^i \in \mathbf{W}$ .
We have $K$ classes of our text-generative models, that generated fragments of text in $\mathbf{D}$.

We have $K + 1$ labels, that we will use for multiclass classification, where $0$ - label of human-written fragment, ${1...K}$ - labels that represent corresponding language model, let $\mathbf{C}$ be our set of labels.

Our model $\mathbf{\phi}$ is a superposition of two mappings, $\mathbf{f}$ and $\mathbf{g}$. Mapping $\mathbf{f}$ is responsible for text segmentation. Mapping $\mathbf{g}$ is responsible for classifying obtained fragments.

$$\mathbf{\phi} : \mathbf{g} \circ \mathbf{f}$$


$$\mathbf{\phi}: \mathbb{D} \rightarrow \mathbb{T},$$

where $$\mathbb{T} = \{[t_{s_j}, t_{f_j}, C_j]_{j = 1}^{J} : t_{s_j} = t_{f_{j - 1}}, s_j \in \mathbb{N}_0, f_j \in \mathbb{N}, C_j \in \mathbf{C}\}$$

and where $J$ - number of fragments in segmentation of the document, $t_{s_j}$ - starting index of the $j$-th fragment,  $t_{f_j}$ - finishing index of the $j$-th fragment,  $C_{j}$ - class of the $j$-th fragment.



\subsection{Text fragmentation}
The first step is to divide our text into fragments of different origin.

$$\mathbf{f}: \mathbb{D} \rightarrow \mathbf{T}^*$$

where $$\mathbf{T}^* = \{[t_{s_j}, t_{f_j}]_{j = 1}^{J} : t_{s_j} = t_{f_{j - 1}}, s_j \in \mathbb{N}_0, f_j \in \mathbb{N}\}$$

Thus, $\mathbf{T}^*$ is a set of all possible sequences of non-overlapping blocks of texts, that covers complete document.

We assume that in each document blocks of text with the same origin is big enough. Thus, we search for style changes on sentence levels. Style change indicates that several next sentences staring from the beginning of the style change have different origin. Each sentence receive a label $0$ or $1$. A fragment is a sequence of neighbouring sentences of maximum possible length, that have the same label. We repeat this process for every document in given  set of document. This can be viewed as a binary classification task. Thus, loss function

$$\mathbf{Loss_{frmg}}(x) = \sum_{i=0}^J [-y_i \log p_i - (1 - y_i) \log (1 - p_i)] $$

where $x \in \mathbb{D}$, $y_i$ - predicted label of $x$, $p_i$ - probabilty of that label.


\subsection{Fragment classification}

The second step is to classify each fragment with label from $\mathbf{C}$.
$$\mathbf{g}: \mathbf{T}^* \rightarrow \mathbf{C}$$

We apply that function to every text fragment, obtained from the previous step. This is a classical multi-classfication task. We will use cross-entropy as a loss function:

$$\mathbf{Loss_{cls}}(x) = \sum_{i=0}^K-p(x)\log q(x)$$ 
where $p(x)$ - probability of object $x$ to be actually labelled with $i$,
$q(x)$ -  probability of object $x$ to be labelled with $i$  in prediction.

\subsection{Metrics}
Total loss of the model is calculated as sum of losses on both stages. This is an inner criterion of quality of our model.

$$\mathbf{Loss} = \mathbf{Loss_{cls}} + \mathbf{Loss_{frgm}}$$

Outer criterion of our model can be set as a metric, that compares the boundaries of some fragments with the reference. For If model doesn't detect a particular fragment at all, we set this metric to 0. Otherwise, we will calculate a distance between two vectors with indexes of beginning of the fragment and its endings.
The more the distance, the smallest values we will assign to that pair. 


\section{Computational Experiment}

The goal of computational experiment is to analyse how good baseline solution in classifying fragments of text. RUATD  competition, which provided the dataset, also provided accuracy score for their baseline solutions. One soltuion uses tf-idf + logistic regression approach, the other solution uses fine-tuned BERT. We will use the second approach, but our own solution and compare our results with the ones, suggested by the authours of competition.
% The task is to find the optimal parameters of our model in which certain requirements on the quality of the model will be achieved.
% For text fragmentation we use Gradient Boosting Regression Tree for classifying the sentences, as described in \cite{Kuznetsov}. The optimal parameters are (???).
% %(n_estimators=200, max_depth=4)%
% %by maximization of the Area-Under-Curve classification measure.
% Also we detect outliers: if a sentence is label-classified with some value higher than threshold, we will label it as a machine generated sentence.

% For fragment classification we suggest to use an approach, described in \cite{gritsay2022}, namely RoBERTa-base model for English texts and XLM-RoBERTa-base model for Russian texts.

\subsection{Dataset}

For our experiment we took the data, that was collected for RUATD 2022 competition\cite{ruatd-dataset} for multitask classification. The test dataset contains 215105 short fragments, that were generated by different 13 models. Authors claim that various language models fine-tuned on different tasks: machine translation, paraphrasing, summarization, simplification and unconditional text generation - were used to generate texts. Moreover, the part of the set was annotated automatically by different generative models. Among models there are several versions of ruGPT, mT5 and ruT5 and M-BART.

\subsection{Configuration of algorithm run}

We used only a part of dataset for the sake of decreasing the time of training. For tokenization of sentences pretrained embedding from \texttt{bert-base-multilingual-cased}\footnote{\url{https://github.com/google-research/bert/blob/master/multilingual.md}} were used. The same model was for fine-tuning for our sequence classification task. We trained our model for 3 epochs. For optimization we used AdamW optimization algorithm\cite{adam-w}.


\subsection{List of expected figures and tables}
\begin{enumerate}
    \item Table of train accuracy and validation accuracy on each epoch
    \item Graph of validation / train loss versus epoch - Figure 1
    \item Graph of Precision-Recall Curve for every class - Figure 2
\end{enumerate}

% The dataset, PAN-PC-11\cite{pan-dataset} was intended to use for intrinsic plagiarism detection. We will analyse our task on sentence-level, this will be out tokens. The test collection consists of 4753 documents and is split into 10 folds. Each folds contains 500 documents except for the smaller fold 10. We used it as a benchmark for measuring, how good our model is in fragmenting a document.

% The dataset, used for text classification problem was GPT-2 output dataset\cite{gpt2-dataset}, which allows  to compare data from models with different numbers of parameters and different sampling methods. 

% For evaluating our task we will need to generate our own dataset with document by taking fragments from this dataset and from RUaTD task dataset \cite{ruatd-dataset} and combining them with fragments of human-written text. However, one should be careful when generating a document, we need to pay attention and do not let our generator to take fragments from different domains. 


  
\section{Preliminary report}

We received 56$\%$ of accuracy with 3 epoch of training only using the part of the dataset. It is les than the score, provided by the authors of competition, which is 59$\%$, but this difference is explained by smaller size of the dataset. Also we may suggest that due to increasing loss on validation set, our model started to overfit.

\begin{table}[bhtp]
	\centering
	\caption{Table of train accuracy and validation accuracy on each epoch}
	\label{tbl:space_and_subspace}
	\begin{tabular}{| c | c | c | c | }
		\hline
		Number of epoch & Train. loss & Valid.loss & Valid. accuracy \\
		\hline
		1 & 1.30  & 1.26 & 0.54  \\
        \hline
		2 & 1.07 & 1.27 & 0.56  \\
		\hline
        3 & 0.86  & 1.35 & 0.56  \\
        \hline
	\end{tabular}
\end{table}

\begin{figure}[bhtp]
	\includegraphics[width=\textwidth]{Unknown-2.png}
	\caption{Graph of validation / train loss versus epoch}
	\label{fig:1}
\end{figure}


\begin{figure}[bhtp]
	\includegraphics[width=\textwidth]{Unknown.png}
	\caption{Graph of Precision-Recall Curve for every class}
	\label{fig:2}
\end{figure}


\pagebreak
\bibliographystyle{plain}
\bibliography{references}

\end{document}