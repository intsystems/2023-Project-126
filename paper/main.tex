\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}

\renewcommand{\headeright}{Draft}
\renewcommand{\undertitle}{Draft}

\title{Detection of machine-generated fragments in text}

\author{ Anastasiya Voznyuk \\
	Department of Applied Informatics and Mathematics\\
	Moscow Institute of Physics and Technology\\
	\texttt{vozniuk.ae@phystech.edu} \\
	%\texttt{\href{https://colab.research.google.com/drive/1HBB8002s5_tFgFxGzIZQcBmsOsW6YcMh?usp=sharing} {Project code}} \\
    \\
    \textbf{Andrey Grabovoy} \\
	Moscow Institute of Physics and Technology\\
    Antiplagiat Company \\
	%% Address \\
	\texttt{grabovoy@ap-team.ru}}
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\

\date{}

\renewcommand{\shorttitle}{Detection of machine-generated fragments in text}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Kornilov Nikita},
pdfkeywords={Maximum risk prediction},
}

\begin{document}
\maketitle

\begin{abstract}
	Language models are improving their ability to detect whether the entire text was generated or written by human. This paper considers the problem of detecting machine-generated parts of text in the document. We introduce a model, that can detect such text fragments and distinguish their origin among several language models. To solve this problem we combine two models. The objective of the first one is to divide document into fragments of human-written text and machine-generated text. The second model aims to classify the generated fragments according to the language model from which they were obtained. In the computational experiment, we analyse the quality of such approach on dataset of documents with fragments generated by GPT-2 and GPT-3.
\end{abstract}


\keywords{Text Generation, Detection of Machine-generated text, Neural Authorship Attribution, Machine Learning, Natural Language Processing}

\section{Introduction}

% \cite{EVL:1}
In recent years there has been a rapid development of language models for text generation, transformers in particular, for example GPT-2\cite{gpt2}, GPT-3\cite{gpt3} and BERT\cite{bert}. Problem of detecting machine-generated text has become more relevant recently due to the released ChatGPT model by OpenAI. This model was learnt on massive amounts of data and now is able to provide texts that are hardly distinguishable from human texts. The opposite task of detecting machine-generated text becomes more important due to many possible malicious usages of this technology. One can analyse the whole text or fragments of text.
We will focus on methods of subtracting the text-generated fragments in the set of documents.\\
With increased computational resources past statistical methods were applied to the novel detection problem. One of them was prediction entropy as an indicator of fake text, as described in \cite{relativeentropy}. Also perplexity\cite{perplexity} of the text or frequency of rare bigrams \cite{rare_bigrams} or value of tf-idf \cite{solaiman} can be taken into account. Another approach is to use classifiers that try to label given texts \cite{Kuznetsov}.\\
For a long period recursive neural networks like LSTM were showing best results at solving the problem of detection. At 2018 the mechanism of self-attention\cite{Vaswani} was introduced and transformers have become new state-of-the-art approach. There are such transformer-base models like GPT\cite{gpt}, GPT-2\cite{gpt2}, GPT-3\cite{gpt3} by OpenAI, CTRL\cite{ctrl} and BERT\cite{bert} etc. Every new model has its own features and is usually learnt on larger amounts of data, but attention mechanism always stays. These model can be used in detection of machine-generated text in two ways\cite{solaiman}. The first method is using a language model that searches for artefacts from  methods, which most models are using for generating concise texts \cite{gltr}. Additional training on new data is not required. The second method is fine-tuning based detection: one fine-tunes a language model to “detect itself” with using some stochastic methods, for example top-k, top-p sampling etc.\\
The first step is to divide each document into non-overlapping blocks of sentences. We take the baseline from work\cite{Kuznetsov} on plagiarism detection. Also metrics and construction of loss function from the works about text segmentation problem can be borrowed. We suppose, that within a fragment with artificial text there is a finished idea and similar semantic structure. For example, loss function is described at \cite{ts_loss}, and metrics are described at \cite{ts_metrics}. The next step is to detect the origin of machine-generated text and this can be done with usage of transformer-based models. 
 
\section{Problem statement}
We have set of $n$ documents
$$\mathbf{D} = \bigcup_{i=1}^{n}D^i$$

Each document  $D^i$ consists of $d_i$ tokens $t_1^i, t_2^i, ..., t_{d_i}^i$.

Also we have $K$ classes of our text-generative models, that generated fragments of text in $\mathbf{D}$.

We have $K + 1$ labels, that we will use for multiclass classification, where $0$ - label of human-written fragment, ${1...K}$ - labels that represent corresponding language model, let $\mathbf{C}$ be our set of labels.

Our model $\mathbf{m}$ is a superposition of two mappings, $\mathbf{f}$ and $\mathbf{g}$. Mapping $\mathbf{f}$ is responsible for text segmentation. Mapping $\mathbf{g}$ is responsible for classifying obtained fragments.

$$\mathbf{m} : \mathbf{g} \circ \mathbf{f}$$


$$\mathbf{m}: \mathbf{D} \rightarrow \mathbf{T},$$

where $$\mathbf{T} = \bigcup_{i=1}^{n}\bigcup_{j \in J_i}(t_{s_j}^i, t_{f_j}^i, C_{ij})$$

and where 
\begin{itemize}
    \item $J_i = \{1 ... j_i\}$, where $j_i$ - number of fragments in document $D^i$.
    \item $t_{s_j}^i$ - starting index of the $j$-th fragment in the $i$-th document, $j \in J_i$, $i \in \{1 ... n\}$.
    \item $t_{f_j}^i$ - finishing index of the $j$-th fragment in the $i$-th document, $j \in J_i$, $i \in \{1 ... n\}$.
    \item $C_{ij}$ - class of the $j$-th fragment in the $i$-th document, $C_{ij} \in \mathbf{C}$.
\end{itemize}

\subsection{Text fragmentation}
The first step is to divide our text on fragments of different origin.

$$\mathbf{f}: \mathbf{D} \rightarrow \bigcup_{i=1}^{n}\bigcup_{j \in J_i}(t_{s_j}^i, t_{f_j}^i)$$

We search for style changes on sentence levels. Style change indicates that several next sentences staring from the beginning of the style change have different origin. Each sentence receive a label $0$ or $1$. A fragment is a sequence of neighbouring sentences of maximum possible length, that have the same label. We repeat this process for every $D^i$.

In our case, we use Gradient Boosting Regression Tree for classifying the sentences, as described in \cite{Kuznetsov}.

\subsection{Fragment classification}

The second step is to classify each fragment with label from $\mathbf{C}$.
$$\mathbf{g}: \bigcup_{i=1}^{n}\bigcup_{j \in J_i}(t_{s_j}^i, t_{f_j}^i) \rightarrow \mathbf{T}$$

We suggest to use an approach, described in \cite{gritsay2022}, namely RoBERTa-base model for English texts and XLM-RoBERTa-base model for Russian texts.

\section{Computational experiment}
  
\section{Preliminary report}


\subsection{List of expected figures and tables}

\pagebreak
\bibliographystyle{plain}
\bibliography{references}

\end{document}